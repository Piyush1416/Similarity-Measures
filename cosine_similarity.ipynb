{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cosine similarity.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DfLqB3pfU2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the documents\n",
        "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
        "\n",
        "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
        "\n",
        "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
        "\n",
        "documents = [doc_trump, doc_election, doc_putin]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM2e8D0hf0Jw",
        "colab_type": "text"
      },
      "source": [
        "We have the following 3 texts:\n",
        "\n",
        "Doc Trump (A) : Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin.\n",
        "\n",
        "Doc Trump Election (B) : President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election.\n",
        "\n",
        "Doc Putin (C) : Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career.\n",
        "\n",
        "Since, Doc B has more in common with Doc A than with Doc C, I would expect the Cosine between A and B to be larger than (C and B)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmFQZD4Uf6Lg",
        "colab_type": "text"
      },
      "source": [
        "To compute the cosine similarity, you need the word count of the words in each document. The CountVectorizer or the TfidfVectorizer from scikit learn lets us compute this. The output of this comes as a sparse_matrix.\n",
        "\n",
        "On this, am optionally converting it to a pandas dataframe to see the word frequencies in a tabular format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDoAivTxfldb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "e429e13a-ff3e-4b90-fb8b-4fb3071da305"
      },
      "source": [
        "# Scikit Learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Create the Document Term Matrix\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_vectorizer = CountVectorizer()\n",
        "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
        "doc_term_matrix = sparse_matrix.todense()\n",
        "df = pd.DataFrame(doc_term_matrix, \n",
        "                  columns=count_vectorizer.get_feature_names(), \n",
        "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>after</th>\n",
              "      <th>as</th>\n",
              "      <th>became</th>\n",
              "      <th>by</th>\n",
              "      <th>career</th>\n",
              "      <th>claimed</th>\n",
              "      <th>do</th>\n",
              "      <th>earlier</th>\n",
              "      <th>election</th>\n",
              "      <th>elections</th>\n",
              "      <th>friend</th>\n",
              "      <th>friends</th>\n",
              "      <th>had</th>\n",
              "      <th>he</th>\n",
              "      <th>his</th>\n",
              "      <th>in</th>\n",
              "      <th>interference</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>lost</th>\n",
              "      <th>minister</th>\n",
              "      <th>mr</th>\n",
              "      <th>no</th>\n",
              "      <th>nothing</th>\n",
              "      <th>of</th>\n",
              "      <th>outcome</th>\n",
              "      <th>parties</th>\n",
              "      <th>political</th>\n",
              "      <th>post</th>\n",
              "      <th>president</th>\n",
              "      <th>prime</th>\n",
              "      <th>putin</th>\n",
              "      <th>republican</th>\n",
              "      <th>russia</th>\n",
              "      <th>says</th>\n",
              "      <th>served</th>\n",
              "      <th>some</th>\n",
              "      <th>support</th>\n",
              "      <th>the</th>\n",
              "      <th>though</th>\n",
              "      <th>to</th>\n",
              "      <th>trump</th>\n",
              "      <th>vladimir</th>\n",
              "      <th>was</th>\n",
              "      <th>who</th>\n",
              "      <th>winning</th>\n",
              "      <th>witchhunt</th>\n",
              "      <th>with</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc_trump</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_election</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_putin</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              after  as  became  by  career  ...  was  who  winning  witchhunt  with\n",
              "doc_trump         1   0       1   0       0  ...    0    0        1          0     1\n",
              "doc_election      0   0       0   1       0  ...    1    1        0          1     1\n",
              "doc_putin         0   1       1   0       1  ...    0    0        0          0     0\n",
              "\n",
              "[3 rows x 48 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X7QCm6agDAf",
        "colab_type": "text"
      },
      "source": [
        "Even better, I could have used the TfidfVectorizer() instead of CountVectorizer(), because it would have downweighted words that occur frequently across docuemnts.\n",
        "\n",
        "Then, use cosine_similarity() to get the final output. It can take the document term matri as a pandas dataframe as well as a sparse matrix as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVF6Appif-hC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1a5a80dd-1a04-4544-b969-093f293a8075"
      },
      "source": [
        "# Compute Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(df, df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.51480485 0.38890873]\n",
            " [0.51480485 1.         0.38829014]\n",
            " [0.38890873 0.38829014 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM1YvsEfgcEF",
        "colab_type": "text"
      },
      "source": [
        "# **Soft Cosine Similarity**\n",
        "\n",
        "Suppose if you have another set of documents on a completely different topic, say ‘food’, you want a similarity metric that gives higher scores for documents belonging to the same topic and lower scores when comparing docs from different topics.\n",
        "\n",
        "In such case, we need to consider the semantic meaning should be considered. That is, words similar in meaning should be treated as similar. For Example, ‘President’ vs ‘Prime minister’, ‘Food’ vs ‘Dish’, ‘Hi’ vs ‘Hello’ should be considered similar. For this, converting the words into respective word vectors, and then, computing the similarities can address this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6G5zOkgHCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define 3 additional documents\n",
        "doc_soup = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
        "\n",
        "doc_noodles = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
        "\n",
        "doc_dosa = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
        "\n",
        "documents = [doc_trump, doc_election, doc_putin, doc_soup, doc_noodles, doc_dosa]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fhHzo-uhuWB",
        "colab_type": "text"
      },
      "source": [
        "To get the word vectors, you need a word embedding model. Let’s download the FastText model using gensim’s downloader api."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WI5DPx8hhtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "104573a2-85c6-4e9a-a9de-3768b842d8dc"
      },
      "source": [
        "import gensim\n",
        "# upgrade gensim if you can't import softcossim\n",
        "from gensim.matutils import softcossim \n",
        "from gensim import corpora\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "print(gensim.__version__)\n",
        "#> '3.6.0'\n",
        "\n",
        "# Download the FastText model\n",
        "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.0\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDTuncK7j-_y",
        "colab_type": "text"
      },
      "source": [
        "To compute soft cosines, you need the dictionary (a map of word to unique id), the corpus (word counts) for each sentence and the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHR1fjFchyMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b62fac18-05ad-4e28-9370-d613e44ca2d6"
      },
      "source": [
        "# Prepare a dictionary and a corpus.\n",
        "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
        "print(dictionary)\n",
        "\n",
        "# Prepare the similarity matrix\n",
        "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
        "\n",
        "# Convert the sentences into bag-of-words vectors.\n",
        "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
        "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
        "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
        "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
        "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
        "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
        "\n",
        "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(107 unique tokens: ['after', 'became', 'election', 'friends', 'he']...)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNf2bFTekVge",
        "colab_type": "text"
      },
      "source": [
        "If you want the soft cosine similarity of 2 documents, you can just call the softcossim() function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIXyXktHkGh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd8cfa8-fb8e-440b-89a3-43fcf5308950"
      },
      "source": [
        "# Compute soft cosine similarity\n",
        "print(softcossim(sent_1, sent_2, similarity_matrix))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5842470477718544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws-3YOCwkewK",
        "colab_type": "text"
      },
      "source": [
        "But, I want to compare the soft cosines for all documents against each other. So, create the soft cosine similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4rClFUnkZa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e21365ae-b9a5-4bb7-c79c-e46a40f6db85"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_soft_cossim_matrix(sentences):\n",
        "    len_array = np.arange(len(sentences))\n",
        "    xx, yy = np.meshgrid(len_array, len_array)\n",
        "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
        "    return cossim_mat\n",
        "\n",
        "create_soft_cossim_matrix(sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.58</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.54</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4     5\n",
              "0  1.00  0.58  0.56  0.28  0.34  0.40\n",
              "1  0.58  1.00  0.54  0.25  0.31  0.43\n",
              "2  0.56  0.54  1.00  0.19  0.25  0.36\n",
              "3  0.28  0.25  0.19  1.00  0.50  0.38\n",
              "4  0.34  0.31  0.25  0.50  1.00  0.56\n",
              "5  0.40  0.43  0.36  0.38  0.56  1.00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIcIgTLdkjT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}